{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from random import shuffle\n",
    "import gpytorch\n",
    "from tqdm import tqdm\n",
    "from gpytorch.models import ApproximateGP, ExactGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import UnwhitenedVariationalStrategy, VariationalStrategy\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.mnist.MNIST(\"mnist\", download=True, \n",
    "                                           transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dataset.targets\n",
    "class1_inds = torch.where(targets==0)[0]\n",
    "class2_inds = torch.where(targets==4)[0]\n",
    "inds = torch.cat([class1_inds, class2_inds])\n",
    "inds = inds[torch.randperm(len(inds))]\n",
    "targets = targets[inds]\n",
    "targets = (targets > 0).int()\n",
    "data = dataset.data[inds]\n",
    "n_train = int(len(data)*0.2)\n",
    "n_test = len(data) - n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape((-1, 784)).float()\n",
    "std = data.std(dim=0)\n",
    "std[std==0] = torch.ones_like(std[std==0])\n",
    "data = (data - data.mean(dim=0))/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data[:n_train].float()\n",
    "test_x = data[n_train:].float()\n",
    "\n",
    "train_y = targets[:n_train]\n",
    "test_y = targets[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INDUCING_POINTS = train_x.shape[0]\n",
    "\n",
    "class GPClassificationModel(ApproximateGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(N_INDUCING_POINTS)\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, train_x,\n",
    "            variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=784))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "\n",
    "model = GPClassificationModel(train_x)\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=784))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7257695198059082: 100%|██████████| 100/100 [10:24<00:00,  6.25s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "rsub() received an invalid combination of arguments - got (Tensor, MultivariateNormal), but expected one of:\n * (Tensor input, Tensor other, *, Number alpha)\n * (Tensor input, Number other, Number alpha)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-141dd80db063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__rsub__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rdiv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: rsub() received an invalid combination of arguments - got (Tensor, MultivariateNormal), but expected one of:\n * (Tensor input, Tensor other, *, Number alpha)\n * (Tensor input, Number other, Number alpha)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the number of training datapoints\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "iters = 100\n",
    "\n",
    "pbar = tqdm(range(iters))\n",
    "for _ in pbar:\n",
    "\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_description(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianLikelihood(\n",
       "  (noise_covar): HomoskedasticNoise(\n",
       "    (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2b06f94a48ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgrad_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mgrad_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parametrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         inputs, allow_unused)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "num_parametrs = sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "hessian = torch.zeros(num_parametrs, num_parametrs).cpu()\n",
    "model.zero_grad()\n",
    "output = model(train_x)\n",
    "loss = -mll(output, train_y)\n",
    "\n",
    "grad_list = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "grad_i = torch.cat([g.view(-1) for g in grad_list]).cpu()\n",
    "for i in range(0, num_parametrs):\n",
    "    hessian[i] = torch.cat(\n",
    "        [g.view(-1) for g in torch.autograd.grad(grad_i[i], model.parameters(), create_graph=True,\n",
    ")]).cpu()\n",
    "hessian_evals[batch_ind] = np.linalg.eigvalsh(hessian.detach().cpu().numpy())\n",
    "del hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([-0.5906]),\n",
       " tensor(0.1245, grad_fn=<SoftplusBackwardBackward>),\n",
       " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.3631e-44,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  5.6052e-45,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5302e-42,  0.0000e+00,\n",
       "           0.0000e+00,  1.4069e-42,  1.3102e-42,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -3.3631e-44,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -1.2357e-31, -9.3394e-32, -1.2447e-32,\n",
       "          -1.3907e-31,  8.1128e-38,  1.3480e-36, -1.4186e-37, -4.3356e-37,\n",
       "          -6.7710e-38, -2.0002e-38, -4.2193e-42,  5.6052e-45,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4013e-45, -7.0271e-32,\n",
       "          -1.3161e-23, -1.8556e-22, -5.0855e-24, -7.5597e-32, -5.0088e-32,\n",
       "          -3.2194e-32, -4.0145e-28, -2.5911e-24, -2.6829e-24, -6.0910e-24,\n",
       "          -3.2570e-25, -5.5571e-34, -1.2075e-38,  1.0115e-39,  1.4013e-45,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -9.0757e-34, -1.1472e-25, -1.6244e-24, -1.5264e-23,\n",
       "          -1.5601e-24, -1.1961e-35, -4.5736e-34, -6.1341e-32, -9.0675e-26,\n",
       "          -9.8791e-25, -1.8939e-24, -1.0099e-24, -1.3851e-25, -3.0376e-34,\n",
       "          -7.9665e-39, -1.9235e-40, -1.4013e-45,  0.0000e+00,  4.4842e-44,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -3.6434e-44, -1.4013e-45,  2.8026e-45, -3.1272e-32,\n",
       "          -1.0848e-24, -1.4837e-23, -9.2616e-25, -3.2326e-23, -5.5455e-25,\n",
       "          -3.8238e-27, -1.7176e-32, -5.7114e-27, -2.2003e-24, -1.1288e-24,\n",
       "          -7.8167e-25, -4.3937e-26, -8.3425e-35, -1.6669e-39, -1.4099e-40,\n",
       "           2.8643e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.2969e-43,\n",
       "          -2.3822e-43, -1.3437e-33, -3.4089e-33, -4.1446e-25, -3.3730e-24,\n",
       "          -1.6744e-24, -4.4898e-24, -8.7970e-25, -1.8239e-26, -4.2699e-33,\n",
       "          -6.6720e-25, -5.5569e-25, -3.2687e-25, -1.0421e-24, -1.5995e-34,\n",
       "          -3.8976e-38, -2.0966e-39, -3.4668e-42,  0.0000e+00,  4.3440e-43,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.6052e-45,\n",
       "           0.0000e+00,  0.0000e+00, -4.1218e-41, -2.5097e-41, -8.1475e-32,\n",
       "          -2.1315e-33, -1.0240e-25, -5.6603e-25, -7.2374e-25, -1.2835e-24,\n",
       "           4.3424e-26,  5.4467e-26, -4.4363e-28, -1.8073e-25, -8.1453e-25,\n",
       "          -5.4190e-24, -3.9978e-24, -2.0363e-33,  9.7471e-38, -1.5561e-39,\n",
       "           1.0797e-41,  1.4013e-45,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2510e-43,  0.0000e+00,\n",
       "          -1.4013e-45, -1.6764e-40, -8.2713e-33, -1.0205e-28, -1.8480e-24,\n",
       "          -1.4530e-27, -1.3822e-24, -1.9147e-25, -4.9913e-25,  3.4414e-26,\n",
       "          -9.8243e-27, -9.1556e-24, -1.0538e-24, -1.7908e-24, -1.7861e-25,\n",
       "          -1.1211e-32,  2.1097e-37, -4.8935e-41, -1.7718e-40,  1.4013e-45,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  3.5873e-43,  0.0000e+00,  2.9988e-42, -6.3733e-40,\n",
       "          -1.5776e-33, -1.5402e-27, -3.9190e-24, -1.9650e-25, -2.1284e-24,\n",
       "          -7.4328e-26, -5.7721e-25,  4.9190e-27, -1.4321e-24, -1.9727e-25,\n",
       "          -9.3522e-25, -5.9689e-24, -6.4207e-34, -1.1491e-33, -6.7836e-33,\n",
       "          -3.0857e-41, -3.6004e-39,  2.5854e-42,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.2599e-43,\n",
       "          -5.1007e-43, -1.0263e-41, -5.0787e-40, -1.4972e-32, -6.7011e-26,\n",
       "          -2.4886e-24, -8.3333e-25, -3.2393e-25, -1.6412e-25, -9.2544e-26,\n",
       "           2.5644e-40, -2.6510e-24,  1.0233e-25, -1.0468e-26, -3.2384e-24,\n",
       "          -2.0429e-31, -5.7675e-36, -1.5286e-33, -1.2702e-33, -2.4843e-39,\n",
       "           3.1641e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -2.8026e-45,  5.3008e-33,\n",
       "           2.1307e-32, -8.0058e-26, -4.0376e-24, -1.6294e-25, -1.9677e-24,\n",
       "          -6.0709e-23,  1.5800e-26,  1.0502e-26, -2.3823e-33, -1.5769e-24,\n",
       "          -1.6845e-25,  1.4714e-26, -3.7155e-24, -2.0670e-31, -5.8687e-36,\n",
       "          -1.7354e-33, -7.6622e-32, -1.1934e-39,  4.2039e-45,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -1.4706e-31, -1.1069e-32, -2.3700e-25, -6.2794e-25,\n",
       "          -6.0450e-27, -2.7984e-25, -1.9880e-24, -1.6172e-23, -1.6061e-24,\n",
       "          -4.6403e-25, -6.4048e-24, -1.2749e-23, -6.3419e-27, -5.5733e-24,\n",
       "          -4.9407e-27, -1.4007e-31, -3.3618e-32, -4.1287e-32, -1.3210e-32,\n",
       "           1.2966e-41,  1.4013e-44,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -9.8920e-33,\n",
       "          -6.7044e-33, -1.2749e-24, -5.0128e-24, -2.7266e-24, -2.3185e-25,\n",
       "          -9.4723e-29, -9.5027e-29,  1.4327e-25,  2.0501e-25, -2.7595e-25,\n",
       "           3.7262e-27, -6.2639e-26, -2.8778e-24, -1.1505e-25, -1.7947e-25,\n",
       "          -1.7863e-25,  6.0291e-34,  2.4159e-33, -3.3853e-32,  5.6052e-45,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -4.8925e-41, -2.4424e-39, -7.6477e-27,\n",
       "          -1.6776e-23, -3.8534e-23, -4.2759e-24, -1.1584e-23, -1.4357e-23,\n",
       "          -8.6447e-24, -1.0944e-24, -4.2116e-24, -1.2465e-27, -2.1701e-25,\n",
       "          -9.2632e-24, -6.1698e-27, -2.6914e-24, -2.9164e-25, -4.5106e-33,\n",
       "          -1.2764e-33,  1.3644e-33, -4.2039e-45,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  8.4078e-44,\n",
       "           1.4013e-45, -2.2293e-39, -6.6208e-39, -1.9752e-24, -2.6988e-23,\n",
       "          -2.6731e-24, -2.2888e-32,  2.1640e-26,  8.6520e-27,  8.6284e-26,\n",
       "          -1.2410e-24, -1.0617e-24, -4.9108e-25, -8.3037e-25, -6.5046e-35,\n",
       "          -6.4285e-33, -6.4195e-36, -2.7257e-35, -2.0552e-33, -1.1210e-44,\n",
       "          -1.0286e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1210e-44, -1.2676e-39,\n",
       "          -1.9751e-39, -4.8104e-33, -1.2526e-35, -3.3589e-34, -4.4043e-34,\n",
       "          -4.4058e-35, -7.9962e-26,  1.8898e-25, -4.3521e-24, -4.2087e-26,\n",
       "          -1.2674e-26, -1.2156e-24, -1.1578e-35, -6.3831e-33, -6.6209e-36,\n",
       "          -8.0486e-34, -4.9636e-35,  3.1319e-42,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  2.8026e-45,  1.0370e-42, -1.2480e-39, -2.2243e-32,\n",
       "          -4.0990e-32, -6.4640e-36, -3.6930e-37, -5.6519e-32, -8.8004e-26,\n",
       "           1.3565e-25, -7.2122e-24, -1.0205e-25, -1.6660e-23,  1.0722e-32,\n",
       "          -8.3038e-36, -1.7284e-32,  3.5322e-40, -3.7225e-34, -4.4768e-32,\n",
       "           3.1347e-42,  1.3705e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           6.6056e-41, -3.1726e-39, -6.1577e-40, -3.8069e-33, -1.6623e-33,\n",
       "          -6.4874e-36, -2.6056e-35,  1.3088e-25, -5.5352e-27, -9.0701e-24,\n",
       "           1.2163e-25, -6.8624e-24, -2.1322e-32, -7.6920e-32, -3.8902e-33,\n",
       "          -3.6681e-35, -4.5486e-32, -5.7360e-32,  0.0000e+00,  4.0498e-43,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  3.0829e-43, -8.7581e-43,  3.2482e-42, -3.9097e-40,\n",
       "          -3.0211e-40, -7.3747e-38, -1.1954e-31, -9.4891e-33, -3.6787e-25,\n",
       "           8.8245e-26, -5.1839e-25, -2.0950e-24, -1.2678e-27, -4.8790e-24,\n",
       "           3.4723e-33, -6.2681e-35, -3.3927e-35, -1.2352e-32, -1.4305e-31,\n",
       "           1.4013e-45,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.3631e-44,\n",
       "           0.0000e+00,  2.5546e-42,  4.5991e-42, -2.5214e-39, -1.1530e-39,\n",
       "          -1.5888e-38, -1.1252e-31, -3.8435e-25,  2.9045e-26, -4.5296e-24,\n",
       "          -7.2840e-25, -4.3693e-24, -6.1400e-25, -2.6628e-35, -4.7800e-35,\n",
       "          -3.2656e-32, -3.2962e-31, -1.4013e-45,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4013e-45,\n",
       "          -1.4517e-42, -1.3524e-39, -5.6732e-39, -6.3575e-39, -1.5318e-25,\n",
       "           2.2444e-25, -5.4312e-27, -2.8002e-26, -3.3819e-26, -1.6327e-23,\n",
       "          -6.4036e-27, -1.0064e-32, -1.2485e-34, -2.8410e-32,  2.8026e-45,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  5.6052e-44,  2.6204e-43,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.0256e-43,\n",
       "           1.1210e-44,  2.9231e-42, -1.3048e-25,  7.7109e-25,  5.0720e-26,\n",
       "          -1.4300e-23, -8.9195e-25, -1.0346e-23, -8.0143e-25, -7.4085e-36,\n",
       "          -3.6634e-36,  3.3211e-42,  0.0000e+00,  0.0000e+00,  4.6383e-43,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  4.2039e-44,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -2.8653e-37,  9.9368e-37,  1.4861e-35,\n",
       "          -2.3202e-35, -3.8605e-35, -5.0112e-39,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1210e-44, -5.0447e-44,\n",
       "           2.7886e-43,  5.0447e-44,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           1.9618e-44,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2500, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "output = likelihood(model(test_x)).mean\n",
    "print(((output - test_y)**2).float().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# likelihood.train()\n",
    "\n",
    "# # Use the adam optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# # \"Loss\" for GPs - the marginal log likelihood\n",
    "# # num_data refers to the number of training datapoints\n",
    "# mll = gpytorch.mlls.VariationalELBO(likelihood, model, train_y.numel())\n",
    "# batch_size = 1000\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# epochs = 10\n",
    "\n",
    "# for _ in range(epochs):\n",
    "#     model.train()\n",
    "#     likelihood.train()\n",
    "#     pbar = tqdm(enumerate(dataloader))\n",
    "#     for i, (x, y) in pbar:\n",
    "#         # Zero backpropped gradients from previous iteration\n",
    "#         optimizer.zero_grad()\n",
    "#         # Get predictive output\n",
    "#         output = model(x)\n",
    "#         # Calc loss and backprop gradients\n",
    "#         loss = -mll(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         pbar.set_description(f\"Loss: {loss.item()}\")\n",
    "#     model.eval()\n",
    "#     likelihood.eval()\n",
    "#     output = likelihood(model(test_x)).probs\n",
    "#     preds = (output > 0.5).int()\n",
    "#     print((preds == test_y).int().float().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = likelihood(model(test_x)).mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994, 0.4994,\n",
       "        0.4994], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1368, -0.1761,  0.1023, -0.0733,  0.0717,  0.0811,  0.0878,  0.0732,\n",
       "         -0.1124, -0.1154, -0.1324,  0.0951, -0.1370,  0.0974, -0.2112, -0.1434,\n",
       "          0.1032, -0.1013, -0.1157,  0.0930,  0.0974, -0.0584, -0.0919, -0.1239,\n",
       "          0.1106, -0.1379, -0.0738, -0.1274, -0.0807, -0.1034,  0.0933, -0.0647,\n",
       "         -0.0940,  0.0871, -0.2157,  0.1238, -0.1177, -0.0557,  0.0701,  0.0805,\n",
       "          0.1446, -0.0841, -0.0334,  0.0900,  0.0997, -0.0858, -0.0940, -0.1104,\n",
       "          0.1083,  0.1334, -0.1071, -0.0880, -0.0764, -0.0512,  0.1084, -0.2025,\n",
       "         -0.1365, -0.0738,  0.1198, -0.0875,  0.0600, -0.1236,  0.0944,  0.1248,\n",
       "          0.1111, -0.1005, -0.0695,  0.0986,  0.1098,  0.1341, -0.1354, -0.1103,\n",
       "          0.1783, -0.1007, -0.0716, -0.1889, -0.1094,  0.1169,  0.1228, -0.0898,\n",
       "          0.0896, -0.0814,  0.1196, -0.0713, -0.0922, -0.1334,  0.1054,  0.1108,\n",
       "          0.1120, -0.1139,  0.1040,  0.1141, -0.0894, -0.1146, -0.0645, -0.0994,\n",
       "         -0.1041,  0.1142,  0.1139, -0.0959, -0.0926,  0.0749, -0.1188, -0.1195,\n",
       "          0.1200,  0.1034, -0.1796, -0.0954, -0.1230,  0.0936,  0.1257, -0.1153,\n",
       "         -0.1102, -0.1145, -0.0405, -0.1314, -0.0811,  0.0581,  0.0903, -0.0671,\n",
       "         -0.0866,  0.1176, -0.0720,  0.0921,  0.0915,  0.0743,  0.0885, -0.0623,\n",
       "         -0.1869, -0.1191,  0.1390, -0.0620,  0.0928, -0.1246,  0.1821, -0.0770,\n",
       "         -0.0962,  0.1055, -0.1002, -0.0923,  0.1205, -0.2014, -0.0873, -0.0847,\n",
       "         -0.0857,  0.1159, -0.0866,  0.1185,  0.0906, -0.1394,  0.1391,  0.1227,\n",
       "          0.1105,  0.0967, -0.1154,  0.0962, -0.1144, -0.0779, -0.1171, -0.0916,\n",
       "          0.0758, -0.1054,  0.0881,  0.1048,  0.2180,  0.1081,  0.1720,  0.0898,\n",
       "         -0.1047,  0.1286,  0.1141, -0.0763,  0.0831, -0.1215, -0.1260,  0.0729,\n",
       "          0.0845,  0.0717, -0.1194, -0.1274, -0.1793, -0.1387, -0.0814, -0.0949,\n",
       "          0.1219,  0.0932,  0.0976, -0.1257,  0.1795,  0.1035,  0.0822,  0.1280,\n",
       "         -0.0984,  0.1232,  0.0518, -0.1238, -0.1018, -0.0836, -0.0872, -0.0789,\n",
       "          0.1175, -0.0790,  0.0947,  0.0827,  0.0954, -0.1944,  0.0855, -0.0939,\n",
       "          0.1281,  0.0884, -0.1225, -0.1130, -0.1121, -0.1066,  0.1000,  0.1122,\n",
       "         -0.0729,  0.0992, -0.0913,  0.0902,  0.1129,  0.0606,  0.1041, -0.0839,\n",
       "          0.1195, -0.0857,  0.0925, -0.1056,  0.1142, -0.0925, -0.1336,  0.0929,\n",
       "         -0.0867, -0.1051,  0.1373, -0.1006, -0.1147,  0.1217,  0.1053,  0.1116,\n",
       "         -0.1677, -0.1136, -0.0807,  0.0935, -0.0831, -0.1458,  0.1053, -0.1908,\n",
       "         -0.1066,  0.1168,  0.0873, -0.1176,  0.0709,  0.0878, -0.0874, -0.0756,\n",
       "         -0.1378, -0.1240,  0.0811, -0.0893,  0.1162, -0.1136,  0.1307, -0.0949,\n",
       "          0.1126,  0.1149, -0.0974,  0.0913, -0.0954,  0.1041,  0.0763,  0.0787,\n",
       "          0.0959, -0.1320,  0.1115,  0.1111,  0.1186,  0.1167, -0.1126, -0.0771,\n",
       "         -0.1048,  0.0713,  0.2099,  0.0983,  0.1784,  0.1756,  0.1880, -0.1146,\n",
       "         -0.1015, -0.1058, -0.1198, -0.1399, -0.0954,  0.2173, -0.0878,  0.1595,\n",
       "          0.0681, -0.1669,  0.1119, -0.0930,  0.0994,  0.0989,  0.1157,  0.1722,\n",
       "         -0.1331, -0.0893, -0.0499,  0.1082, -0.0880, -0.0844,  0.1159, -0.0756,\n",
       "         -0.0765, -0.1069, -0.0859, -0.0881,  0.1142,  0.1143, -0.0784, -0.0839,\n",
       "         -0.0503,  0.1023, -0.0854, -0.0961,  0.0749,  0.0798, -0.0697, -0.0796,\n",
       "         -0.0928, -0.1033,  0.0968, -0.1266, -0.1203, -0.0931, -0.0960, -0.0767,\n",
       "         -0.1060, -0.0959, -0.1237, -0.1725, -0.1300,  0.0777,  0.0610, -0.0854,\n",
       "          0.1106, -0.1150,  0.1013, -0.0868, -0.0507,  0.2197,  0.0834, -0.0932,\n",
       "          0.0650, -0.0728, -0.0831, -0.1422,  0.1157,  0.0943, -0.0980,  0.1131,\n",
       "          0.1337,  0.0731, -0.0848,  0.0753, -0.0575, -0.1160,  0.0679, -0.1200,\n",
       "         -0.0671,  0.1264,  0.1332, -0.0639,  0.1005, -0.0915,  0.1533,  0.1535,\n",
       "         -0.1980, -0.0987, -0.1121,  0.1179,  0.1192, -0.1126,  0.0985,  0.1740,\n",
       "         -0.0807,  0.0849, -0.0628,  0.0955, -0.1117, -0.1107,  0.0941, -0.1713,\n",
       "          0.0990,  0.1164,  0.0879,  0.1980,  0.1314,  0.1160, -0.0972, -0.0542,\n",
       "         -0.1082,  0.1136,  0.0998,  0.0865, -0.0971, -0.1443, -0.1333, -0.1092,\n",
       "          0.1014,  0.0859, -0.1256, -0.0918,  0.0939, -0.1984,  0.1535, -0.1173,\n",
       "         -0.1190, -0.1024,  0.1284,  0.1050, -0.1056, -0.1161,  0.1098,  0.1067,\n",
       "          0.1864,  0.0705, -0.1004, -0.1272, -0.1173,  0.0981,  0.0970, -0.1376,\n",
       "         -0.0885, -0.1173, -0.0977,  0.1259,  0.0951,  0.1173,  0.0837, -0.0626,\n",
       "         -0.0715,  0.0780, -0.1837,  0.0995,  0.0977, -0.1164, -0.1254, -0.1249,\n",
       "         -0.1073, -0.0984, -0.1316, -0.0650,  0.1091, -0.1172,  0.1200, -0.0743,\n",
       "         -0.0823,  0.1329,  0.1186, -0.0937, -0.0897,  0.0682, -0.1367, -0.0918,\n",
       "         -0.1156,  0.0776,  0.0905, -0.0473, -0.1148,  0.0760, -0.0666, -0.1827,\n",
       "          0.0788,  0.1025,  0.0774, -0.0604,  0.1076,  0.1145, -0.0644, -0.0891,\n",
       "          0.0943,  0.0987, -0.0870, -0.0488, -0.1032,  0.1167, -0.0886,  0.1021,\n",
       "          0.1361,  0.0818, -0.1079,  0.1005, -0.1189, -0.0835,  0.1641,  0.0799,\n",
       "          0.0841, -0.1035, -0.0848,  0.1058, -0.1044,  0.0991,  0.0972, -0.0869,\n",
       "          0.1993,  0.0655,  0.1095,  0.1065, -0.1351,  0.1401,  0.1218,  0.1677,\n",
       "          0.0780,  0.0852,  0.0858,  0.0987,  0.1193,  0.1117, -0.0725, -0.1128,\n",
       "          0.1244, -0.1247, -0.0852, -0.0781,  0.1185,  0.0983, -0.1070, -0.1109,\n",
       "          0.1125, -0.1211,  0.0978, -0.1157, -0.0603,  0.0610,  0.1793, -0.0992,\n",
       "          0.1344, -0.0925, -0.1312,  0.1219, -0.0764, -0.1031, -0.0926,  0.0875,\n",
       "         -0.1042, -0.0657,  0.1339, -0.1108, -0.0934, -0.0902,  0.1311, -0.1160,\n",
       "         -0.0546, -0.0743, -0.0939,  0.0770,  0.1308,  0.1136, -0.1278,  0.0640,\n",
       "          0.1215, -0.0785, -0.1337,  0.1143,  0.1749, -0.1021, -0.0775,  0.1768,\n",
       "         -0.1114,  0.2744, -0.0963, -0.0725,  0.0760, -0.0979, -0.1231,  0.1235,\n",
       "          0.1061, -0.0967, -0.0938, -0.1040, -0.0812,  0.1048, -0.0930,  0.0968,\n",
       "          0.1079,  0.1090,  0.0733,  0.0981,  0.1222, -0.1301,  0.1040, -0.0739,\n",
       "         -0.1164,  0.0972, -0.1029, -0.0825,  0.1119, -0.0881,  0.1072,  0.1263],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.9905, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.9871, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.9969,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.9964, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9993, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.9987]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0012], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(-4.1114, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           3.6836e-31, -7.4071e-32,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  6.4530e-34,  0.0000e+00,\n",
       "          -2.8261e-31,  5.0755e-31,  4.9991e-31,  9.9983e-33, -3.0279e-31,\n",
       "          -2.6320e-32, -6.0350e-32,  6.3633e-34, -1.8525e-32,  1.0395e-30,\n",
       "          -2.5821e-31, -1.8099e-32,  1.1897e-31,  3.7823e-31, -2.9692e-32,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  2.7612e-31, -6.7515e-34,  0.0000e+00,  1.2247e-31,\n",
       "           1.3721e-31,  1.0488e-30,  7.9270e-24,  5.5903e-24,  4.8249e-23,\n",
       "           1.4724e-21,  5.4980e-20,  2.3954e-19,  2.3046e-20,  5.2207e-21,\n",
       "           7.4516e-20,  2.3965e-21,  2.4082e-20,  4.9136e-22,  1.7355e-27,\n",
       "           0.0000e+00, -3.1897e-32, -4.9697e-32,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -1.2331e-34, -2.1196e-32, -1.9338e-31,  3.4557e-31,  4.5522e-24,\n",
       "           7.7417e-24,  3.1564e-25,  5.9420e-21,  4.9663e-20,  1.8839e-19,\n",
       "           9.5508e-22,  5.4365e-23,  5.5881e-23,  1.6025e-22,  3.5651e-20,\n",
       "           8.9106e-20,  9.5184e-21, -1.8771e-24,  2.7522e-29,  9.8987e-31,\n",
       "          -1.4086e-32,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -4.5209e-33,  1.2355e-31, -6.5055e-33,  3.8784e-31,\n",
       "          -9.6746e-31,  5.5458e-26,  1.8796e-24,  2.3256e-24,  1.6185e-21,\n",
       "           2.0315e-20,  1.1122e-20,  1.7785e-22,  1.3234e-22,  9.5817e-23,\n",
       "           2.4465e-22,  1.6878e-22,  1.1900e-21,  4.8074e-21,  4.6915e-22,\n",
       "           1.2734e-20,  5.4367e-31,  6.9851e-32,  3.1376e-32,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -6.3136e-33,  1.5165e-31,  2.7325e-35, -8.9874e-31,  2.0855e-24,\n",
       "           1.8745e-22,  1.5727e-20,  6.1467e-22,  6.2160e-20,  5.9553e-22,\n",
       "           1.0987e-21,  5.8045e-21,  1.1915e-21,  2.4251e-20,  8.1419e-21,\n",
       "           1.2435e-22,  1.9565e-20,  4.8110e-21,  2.9867e-20,  8.8850e-23,\n",
       "           1.0821e-32,  1.9872e-32,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  3.6505e-31, -1.8288e-32, -3.1492e-32,\n",
       "          -9.5214e-31,  8.3332e-26,  4.4519e-25,  1.3619e-20,  5.0364e-23,\n",
       "           3.2802e-21,  7.4078e-21,  4.8757e-22,  4.0927e-21,  9.8503e-21,\n",
       "           1.4873e-19,  8.0220e-21,  2.7631e-20,  2.0970e-21,  1.7632e-21,\n",
       "           1.5271e-20,  1.1901e-21,  1.1157e-21,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           1.5484e-31,  1.2539e-31,  9.9248e-33,  2.5535e-30,  5.3278e-24,\n",
       "           2.6247e-21,  9.9869e-21,  1.6799e-19,  9.9142e-21,  2.6351e-20,\n",
       "           2.6977e-21,  1.3490e-22,  7.7211e-21,  1.3090e-19,  2.4469e-21,\n",
       "           1.2666e-19,  5.6975e-21,  2.2643e-22,  4.5456e-20,  1.2542e-22,\n",
       "           2.1523e-21, -2.8479e-31, -5.4471e-33,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -9.4305e-32, -2.9716e-32,\n",
       "           1.0580e-32,  1.8720e-32,  5.6927e-25,  9.1489e-21,  6.9540e-22,\n",
       "           9.6050e-20,  1.1027e-20,  2.8008e-20,  4.1424e-20,  7.6753e-20,\n",
       "           5.5539e-22,  6.0567e-22,  2.9229e-22,  2.8787e-20,  9.1830e-21,\n",
       "           1.0282e-20,  1.8200e-22,  2.4278e-21,  6.8664e-22,  7.4725e-23,\n",
       "           1.9724e-32,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -2.8358e-32, -2.0995e-31,  2.1405e-30,\n",
       "           1.0810e-20,  7.4056e-21,  8.4912e-21,  5.5408e-22,  1.7108e-21,\n",
       "           4.1508e-21,  8.0325e-20,  2.8218e-22,  3.4228e-22,  1.8595e-21,\n",
       "           5.5900e-22,  3.1500e-21,  3.9359e-20,  4.3221e-22,  5.2931e-22,\n",
       "           6.4116e-20,  5.2062e-21,  1.3766e-22, -2.2093e-32,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5124e-31,\n",
       "           0.0000e+00, -1.3501e-31,  1.7336e-29,  9.0337e-21,  4.4864e-20,\n",
       "           6.0062e-21,  5.1531e-22,  3.1988e-21,  1.1412e-19,  2.9037e-20,\n",
       "           2.6093e-23, -2.6213e-23,  3.0502e-22,  1.7786e-21,  1.2386e-20,\n",
       "           1.9430e-20,  1.3689e-21,  1.1027e-21,  6.3272e-20,  4.6881e-21,\n",
       "           1.0773e-22,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  2.5277e-33, -7.3585e-32,  8.3465e-30,\n",
       "           1.1309e-21,  3.5643e-22,  2.0957e-22,  1.9151e-21,  5.4513e-22,\n",
       "           8.4793e-21,  9.4572e-21,  1.6358e-22,  5.5566e-25,  2.9659e-21,\n",
       "           6.1859e-22,  6.6984e-21,  8.0356e-22,  4.3785e-21,  8.8047e-22,\n",
       "           2.7564e-22,  5.6667e-22,  1.4249e-21,  1.3522e-22, -7.0775e-32,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           1.4119e-31, -2.7453e-31,  3.8149e-30,  2.0555e-21,  1.2548e-19,\n",
       "           2.6203e-20,  2.4452e-23,  5.9301e-21,  3.8603e-20,  3.0513e-21,\n",
       "           3.4987e-22,  2.6670e-21, -2.0563e-22,  2.1388e-21,  1.3304e-21,\n",
       "           4.5611e-22,  1.1949e-19,  1.3759e-21,  7.7753e-23,  7.2031e-20,\n",
       "           9.7351e-21, -2.9999e-31,  9.8232e-32,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -6.2623e-33, -9.7342e-31,\n",
       "           2.0112e-28,  1.7064e-20,  9.5381e-21,  5.7945e-22,  6.0461e-21,\n",
       "           3.8969e-21,  2.6499e-19,  6.9351e-22,  1.0297e-22,  1.4431e-21,\n",
       "           2.4845e-22,  2.0729e-22,  1.0604e-21,  3.5540e-21,  1.2215e-19,\n",
       "           2.9530e-22,  2.3642e-22,  1.3646e-19,  1.6770e-20,  0.0000e+00,\n",
       "          -7.7235e-32,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  1.0109e-31,  1.6292e-32,  8.7373e-21,  8.5919e-21,\n",
       "           7.2508e-20,  4.0413e-22,  1.1861e-20,  4.1512e-20,  1.8543e-21,\n",
       "          -1.8645e-22,  1.0621e-20,  2.2270e-24,  1.1166e-21,  5.4672e-21,\n",
       "           2.3765e-21,  3.7805e-21,  3.7393e-20,  1.4484e-22,  9.1486e-23,\n",
       "           3.6533e-20,  8.2350e-23, -1.2251e-31, -3.0839e-32,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9443e-32,\n",
       "           4.0065e-30,  1.1020e-20,  2.2168e-20,  4.3475e-22,  5.5630e-22,\n",
       "           5.0694e-21,  1.7587e-19,  1.7425e-21, -1.3314e-23,  4.2634e-23,\n",
       "           3.6461e-22,  7.3687e-21,  2.3012e-20,  2.4809e-20,  2.9781e-19,\n",
       "           7.2422e-23,  1.7639e-22,  1.2627e-21,  3.9615e-20,  4.9965e-27,\n",
       "          -6.8230e-31,  2.5277e-33,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -1.2405e-32, -6.8935e-31,  1.0977e-20,\n",
       "           1.7610e-20,  2.9680e-22,  5.0651e-23,  1.9734e-21,  5.6434e-21,\n",
       "           9.7687e-24,  5.7758e-22,  5.0082e-23,  1.4571e-21,  8.4881e-21,\n",
       "           3.9814e-22,  2.5286e-20,  8.9626e-22,  1.9470e-22,  5.5927e-22,\n",
       "           3.9825e-21,  1.5241e-26,  8.0888e-27,  5.6198e-31,  1.0664e-31,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -6.8413e-33,  3.7047e-30,  1.9449e-22,  1.0613e-20,  3.1106e-21,\n",
       "           5.3999e-23,  6.9920e-21,  6.5520e-19,  8.3038e-22,  2.6094e-21,\n",
       "           2.0048e-21,  7.9034e-21,  1.4166e-20,  7.9988e-21,  2.5162e-20,\n",
       "           3.2757e-22,  2.7951e-21,  6.4131e-20,  5.0038e-26,  2.8804e-24,\n",
       "           1.3802e-32, -2.2191e-31, -5.6225e-32,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.6034e-32,  4.1634e-30,\n",
       "           2.9835e-30,  2.5868e-21,  1.3487e-19,  4.9716e-23,  3.4681e-21,\n",
       "           4.3844e-20,  2.9255e-21,  5.0096e-20,  4.0539e-20,  6.2439e-21,\n",
       "           3.3079e-21,  3.0834e-22,  5.8327e-22,  1.0630e-21,  3.2125e-21,\n",
       "           9.9442e-21,  2.6815e-24,  3.7053e-24,  4.6859e-33,  2.4251e-33,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  4.1898e-31,  2.9297e-32,  8.8422e-22,\n",
       "           5.5368e-19,  5.2433e-21,  1.0046e-22,  1.3100e-22,  5.8539e-22,\n",
       "           4.6369e-23,  3.7975e-21,  4.8082e-23,  2.1087e-22,  3.3479e-22,\n",
       "           7.4291e-21,  1.8201e-20,  7.1225e-21,  6.5683e-25,  8.9416e-24,\n",
       "           0.0000e+00,  3.7299e-31, -1.8946e-32,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           3.6199e-31, -2.1848e-31,  7.0146e-22,  2.6120e-19,  2.2635e-19,\n",
       "           4.1302e-21,  8.4098e-23,  8.0606e-23,  4.8248e-23,  1.3817e-21,\n",
       "           5.5037e-22,  2.3916e-21,  1.7036e-20,  4.2068e-21,  1.4707e-23,\n",
       "           1.9244e-24,  2.1094e-23, -1.9385e-31,  1.0708e-30,  1.1275e-31,\n",
       "           8.8791e-32,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5365e-32,  4.3587e-31,\n",
       "           3.0343e-23,  7.7401e-22,  8.9161e-20,  8.4282e-20,  3.8007e-20,\n",
       "           2.9953e-22,  4.5419e-21,  5.7351e-21,  1.3985e-21,  8.5218e-20,\n",
       "           1.7106e-21,  5.6143e-25,  2.2455e-27,  1.8420e-24,  7.2146e-31,\n",
       "           9.0153e-31,  2.3881e-33,  2.1920e-32, -1.1070e-34,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  1.0035e-33, -4.4301e-33,  5.2086e-31, -8.6756e-32,\n",
       "           3.2004e-29,  1.4688e-26,  1.5000e-26,  2.5715e-26,  7.6153e-24,\n",
       "           1.4140e-22,  5.3285e-23,  2.4201e-23,  3.1593e-28,  9.6669e-28,\n",
       "           9.8950e-29, -2.1025e-31, -8.3598e-32, -5.4297e-32, -1.2423e-32,\n",
       "          -1.5668e-33,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -7.8660e-32,  4.2525e-31,  2.5957e-32, -1.4016e-30,\n",
       "           7.8884e-31, -1.8129e-31,  4.2169e-31, -1.2750e-31, -8.1221e-32,\n",
       "          -1.0865e-30,  8.2450e-31, -2.4014e-32,  0.0000e+00, -1.7362e-34,\n",
       "           4.4842e-36,  0.0000e+00, -3.3603e-34,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -1.3572e-33,  2.9033e-31,  3.7232e-32,  6.5300e-35,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
